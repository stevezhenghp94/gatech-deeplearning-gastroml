# -*- coding: utf-8 -*-
"""Testing-steve.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QiGd25mgI77wRO3C9HrxmxTSGZZY86IG
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount("/content/drive")
# cd to a folder in your Drive - in my case is this route
# %cd '/content/drive/Othercomputers/Mi portaÌtil/gastroml'

import numpy as np
# data processing, CSV file I / O (e.g. pd.read_csv)
import pandas as pd
import os
import tensorflow as tf
from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import Tokenizer
from keras.models import Model
from keras.layers import Flatten, Dense, LSTM, Dropout, Embedding, Activation
from keras.layers import concatenate, BatchNormalization, Input
from keras.layers import add
from keras.utils import to_categorical, plot_model
from keras.applications.inception_v3 import InceptionV3, preprocess_input
import matplotlib.pyplot as plt  # for plotting data
import cv2
import string

df = pd.read_csv('Kaggle data/final_data.csv')
df.head()

def load_description(data):
	mapping = dict()
	for index, row in data.iterrows():
		mapping[row['image_name']] = row['title']
	return mapping

descriptions = load_description(df)
print(descriptions['miso-butter-roast-chicken-acorn-squash-panzanella'])

def clean_description(desc):
	for img_name, title in desc.items():
		caption = [ch for ch in title if ch not in string.punctuation]
		caption = ''.join(caption)
		caption = caption.split(' ')
		caption = [word.lower() for word in caption if len(word)>1 and word.isalpha()]
		caption = ' '.join(caption)
		desc[img_name] = caption

clean_description(descriptions)
descriptions['miso-butter-roast-chicken-acorn-squash-panzanella']

def to_vocab(desc):
    words = set()
    for key in desc.keys():
        for line in desc[key]:
            words.update(line.split())
    return words
vocab = to_vocab(descriptions)

import glob
images = 'Kaggle data/images/train/'
# Create a list of all image names in the directory
img = glob.glob(images + '*.jpg')
len(img)

img[0:5]

descriptions.items()

#train_img = img[0:5]
train_img = img

# load descriptions of training set in a dictionary. Name of the image will act as ey
def load_clean_descriptions(des, dataset):
	dataset_des = dict()
	for img_name, title in des.items():
		if 'Kaggle data/images/train/' + img_name + '.jpg' in dataset:
			dataset_des[img_name] = title
	return dataset_des

train_descriptions = load_clean_descriptions(descriptions, train_img)
print(len(train_descriptions))

from keras.preprocessing.image import load_img, img_to_array
def preprocess_img(img_path):
	# inception v3 excepts img in 299 * 299 * 3
	img = load_img(img_path, target_size = (299, 299))
	x = img_to_array(img)
	# Add one more dimension
	x = np.expand_dims(x, axis = 0)
	x = preprocess_input(x)
	return x

def encode(image):
	image = preprocess_img(image)
	vec = img_model.predict(image)
	vec = np.reshape(vec, (vec.shape[1]))
	return vec

base_model = InceptionV3(weights = 'imagenet')
img_model = Model(base_model.input, base_model.layers[-2].output)
# run the encode function on all train images and store the feature vectors in a list
encoding_train = {}
counter = 0
for img in train_img:
	print(counter)
	counter += 1
	encoding_train[img[len(images):]] = encode(img)

# list of all training captions
all_train_captions = []
for key, val in train_descriptions.items():
	all_train_captions.append(val)

# consider only words which occur atleast 10 times
vocabulary = vocab
threshold = 1 # you can change this value according to your need
word_counts = {}
for cap in all_train_captions:
	for word in cap.split(' '):
		word_counts[word] = word_counts.get(word, 0) + 1

vocab = [word for word in word_counts if word_counts[word] >= threshold]

# word mapping to integers
ixtoword = {}
wordtoix = {}

ix = 1
for word in vocab:
	wordtoix[word] = ix
	ixtoword[ix] = word
	ix += 1

# find the maximum length of a description in a dataset
max_length = max(len(des.split()) for des in all_train_captions)
max_length

train_descriptions

encoding_train

X1, X2, y = list(), list(), list()
vocab_size = len(vocab) + 1
for img_name, title in train_descriptions.items():
    pic = encoding_train[img_name + '.jpg']
    seq = [wordtoix[word] for word in title.split(' ') if word in wordtoix]
    #print(title)
    #print(seq)
    for i in range(1, len(seq)):
        in_seq, out_seq = seq[:i], seq[i]

        in_seq = pad_sequences([in_seq], maxlen = max_length)[0]
        out_seq = to_categorical([out_seq], num_classes = vocab_size)[0]
        #print(in_seq)
        #print(out_seq)
        # store
        X1.append(pic)
        X2.append(in_seq)
        y.append(out_seq)

X2 = np.array(X2)
X1 = np.array(X1)
y = np.array(y)

# load glove vectors for embedding layer
embeddings_index = {}
golve_path ='Kaggle data/glove.6B.200d.txt'
glove = open(golve_path, 'r', encoding = 'utf-8').read()
for line in glove.split("\n"):
	values = line.split(" ")
	word = values[0]
	indices = np.asarray(values[1: ], dtype = 'float32')
	embeddings_index[word] = indices

emb_dim = 200
emb_matrix = np.zeros((vocab_size, emb_dim))
for word, i in wordtoix.items():
	emb_vec = embeddings_index.get(word)
	if emb_vec is not None:
		emb_matrix[i] = emb_vec
emb_matrix.shape

# define the model
ip1 = Input(shape = (2048, ))
fe1 = Dropout(0.2)(ip1)
fe2 = Dense(256, activation = 'relu')(fe1)
ip2 = Input(shape = (max_length, ))
se1 = Embedding(vocab_size, emb_dim, mask_zero = True)(ip2)
se2 = Dropout(0.2)(se1)
se3 = LSTM(256)(se2)
decoder1 = add([fe2, se3])
decoder2 = Dense(256, activation = 'relu')(decoder1)
outputs = Dense(vocab_size, activation = 'softmax')(decoder2)
model = Model(inputs = [ip1, ip2], outputs = outputs)

model.layers[2].set_weights([emb_matrix])
model.layers[2].trainable = False
model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')
model.fit([X1, X2], y, epochs = 500, batch_size = 256)
# you can increase the number of epochs for better results

def greedy_search(pic):
	start = 'startseq'
	print(pic.shape)
	for i in range(max_length):
		seq = [wordtoix[word] for word in start.split() if word in wordtoix]
		seq = pad_sequences([seq], maxlen = max_length)
		#print(seq)
		#print(seq.shape)
		yhat = model.predict([pic, seq])
		yhat = np.argmax(yhat)
		word = ixtoword[yhat]
		start += ' ' + word
		if word == 'endseq':
			break
	final = start.split()
	final = final[1:-1]
	final = ' '.join(final)
	return final

img_path = 'demo_imgs/6.jpg'
encoded = encode(img_path)
greedy_search(encoded.reshape(1,-1))