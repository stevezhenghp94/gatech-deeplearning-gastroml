{"cells":[{"cell_type":"markdown","metadata":{"id":"EQ83B8T2nC47"},"source":["# Model comparison\n","## Meta pretrain model vs models train on different datasets\n","\n","#### Meta model was train on the 1M recipe dataset\n","#### Custom models were train on Kaggle Food Ingredients and Recipes Dataset with Images\n","https://www.kaggle.com/datasets/pes12017000148/food-ingredients-and-recipe-dataset-with-images?select=Food+Images"]},{"cell_type":"markdown","metadata":{"id":"bGComgkEnC5C"},"source":["#### Configure drive and import libraries\n","\n","The notebook is run on colab, the following command will mount the project folder"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21748,"status":"ok","timestamp":1702060495984,"user":{"displayName":"juan rodrigo reyes","userId":"17113265916897419484"},"user_tz":300},"id":"zQ8q7NO1nC5D","outputId":"259c0493-d4c5-40fa-9419-5170b74e9eba"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/Othercomputers/Mi portátil/gastroml/src\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n","# cd to a folder in your Drive - in my case is this route\n","%cd '/content/drive/Othercomputers/Mi portátil/gastroml/src'"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":17415,"status":"ok","timestamp":1702060513395,"user":{"displayName":"juan rodrigo reyes","userId":"17113265916897419484"},"user_tz":300},"id":"TaD4mOm7nC5G"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import pandas as pd\n","import numpy as np\n","import os\n","from args import get_parser\n","import pickle\n","from model import get_model\n","from torchvision import transforms\n","from utils.output_utils import prepare_output\n","from PIL import Image\n","import time\n","from build_vocab import Vocabulary\n","\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1702060668756,"user":{"displayName":"juan rodrigo reyes","userId":"17113265916897419484"},"user_tz":300},"id":"LPfV_q0onC5I"},"outputs":[],"source":["# code will run in gpu if available and if the flag is set to True, else it will run on cpu\n","use_gpu = False\n","device = torch.device('cuda' if torch.cuda.is_available() and use_gpu else 'cpu')\n","map_loc = None if torch.cuda.is_available() and use_gpu else 'cpu'"]},{"cell_type":"markdown","metadata":{"id":"q_D-iyOPnC5J"},"source":["# Model and vocab definitions and paths"]},{"cell_type":"markdown","metadata":{"id":"-UJV73EInC5K"},"source":["### Constants definitions"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1702060670738,"user":{"displayName":"juan rodrigo reyes","userId":"17113265916897419484"},"user_tz":300},"id":"wiMlniOMnC5M"},"outputs":[],"source":["params_dict = {}\n","model_list = [\"model\", \"modeResnet18\", \"modeResnet101\", \"modelResnet152\", \"modelMeta\"]\n","model_dict = {\"model\":\"Resnet50\", \"modeResnet18\":\"Resnet18\", \"modeResnet101\":\"Resnet101\", \"modelResnet152\":\"Resnet152\",\"modelMeta\":\"Meta\"}\n","model_dir = \"../checkpoints/inversecooking\"\n","data_dir = '../data'\n","vocab_ings_dict = {\"model\":\"recipe1m_vocab_ingrs\", \"modeResnet18\":\"recipe1m_vocab_ingrs\", \"modeResnet101\":\"recipe1m_vocab_ingrs\", \"modelResnet152\":\"recipe1m_vocab_ingrs\",\"modelMeta\":\"ingr_vocab_meta\"}\n","vocab_inst_dict = {\"model\":\"recipe1m_vocab_toks\", \"modeResnet18\":\"recipe1m_vocab_toks\", \"modeResnet101\":\"recipe1m_vocab_toks\", \"modelResnet152\":\"recipe1m_vocab_toks\",\"modelMeta\":\"instr_vocab_meta\"}"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1702060671120,"user":{"displayName":"juan rodrigo reyes","userId":"17113265916897419484"},"user_tz":300},"id":"V3bBnzM0nC5N"},"outputs":[],"source":["train_image_folder = os.path.join(\"../Kaggle data/images\", 'train')\n","test_image_folder = os.path.join(\"../Kaggle data/images\", 'test')\n","val_image_folder = os.path.join(\"../Kaggle data/images\", 'val')\n","\n","train_imgs = os.listdir(train_image_folder)\n","test_imgs = os.listdir(test_image_folder)\n","val_imgs = os.listdir(val_image_folder)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1702060672775,"user":{"displayName":"juan rodrigo reyes","userId":"17113265916897419484"},"user_tz":300},"id":"XIVVwjp3nZEl","outputId":"b2c4e8ac-c617-4f50-b9cb-fe38509d4b04"},"outputs":[{"data":{"text/plain":["1978"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["len(val_imgs)"]},{"cell_type":"markdown","metadata":{"id":"b1NuwivvnC5O"},"source":["### Functions to get vocab, load_model based on model name"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1702060672775,"user":{"displayName":"juan rodrigo reyes","userId":"17113265916897419484"},"user_tz":300},"id":"y2GueXhnnC5O"},"outputs":[],"source":["def get_vocab_pickle(model_name):\n","    \"\"\"Function to get the vocab pickle file for a given model name.\n","    If the model name is \"modelMeta\", the function will return the meta vocab pickle files.\n","    The meta vocab pickle files are pretrained and were downloaded from the original repo.\n","    url: https://github.com/facebookresearch/inversecooking\n","    If the model name is \"model\", the function will return the recipe1m vocab pickle files.\n","    The vocab files were created using the build_vocab.py script with the kaggle dataset.\n","\n","    Args:\n","        model_name (str): Model name. It can be \"model\", \"modelMeta\", \"modeResnet18\", \"modeResnet101\", \"modelResnet152\"\n","\n","    Returns:\n","        vocab, ingrs_vocab, ingr_vocab_size, instrs_vocab_size, output_dim: vocab pickle files and vocab sizes\n","    \"\"\"\n","    vocab_ing_name = vocab_ings_dict[model_name]\n","    vocab_inst_name = vocab_inst_dict[model_name]\n","    if model_name != \"modelMeta\":\n","        ing_vocab_name = \"ingr_vocab\"\n","        vocab_name = \"instr_vocab\"\n","        ingrs_vocab = pickle.load(open(os.path.join(data_dir, f'{vocab_ing_name}.pkl'), 'rb'))\n","        ingrs_vocab = [min(w, key=len) if not isinstance(w, str) else w for w in ingrs_vocab.idx2word.values()]\n","        vocab = pickle.load(open(os.path.join(data_dir, f'{vocab_inst_name}.pkl'), 'rb')).idx2word\n","\n","    else:\n","        ing_vocab_name = \"ingr_vocab_meta\"\n","        vocab_name = \"instr_vocab_meta\"\n","        ingrs_vocab = pickle.load(open(os.path.join(data_dir, 'ingr_vocab_meta.pkl'), 'rb'))\n","        vocab = pickle.load(open(os.path.join(data_dir, 'instr_vocab_meta.pkl'), 'rb'))\n","\n","    pickle.dump(ingrs_vocab, open(f'../data/f{ing_vocab_name}.pkl', 'wb'))\n","    pickle.dump(vocab, open(f'../data/f{vocab_name}.pkl', 'wb'))\n","\n","    ingr_vocab_size = len(ingrs_vocab)\n","    instrs_vocab_size = len(vocab)\n","    output_dim = instrs_vocab_size\n","    return vocab, ingrs_vocab, ingr_vocab_size, instrs_vocab_size, output_dim"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1702060673374,"user":{"displayName":"juan rodrigo reyes","userId":"17113265916897419484"},"user_tz":300},"id":"RFa9KRpznC5P"},"outputs":[],"source":["# for model_name in model_list:\n","#     print(model_name)\n","#     vocab, ingrs_vocab, ingr_vocab_size, instrs_vocab_size, output_dim = get_vocab_pickle(model_name)\n","#     print(ingr_vocab_size, instrs_vocab_size, output_dim)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1702060673817,"user":{"displayName":"juan rodrigo reyes","userId":"17113265916897419484"},"user_tz":300},"id":"DEhTSGk4nC5Q"},"outputs":[],"source":["import sys; sys.argv=['']; del sys\n","def load_model(model_name, model_dir, map_loc):\n","    \"\"\"Function to load a model given a model name, a model directory and a map location.\n","    The map location is the device where the model will be loaded.\n","    The model directory is the directory where the model is stored.\n","    The model name is the name of the model to be loaded.\n","    The meta model is the model trained with the meta vocab pickle files and\n","    downloaded from the original repo. This model was trained with the original dataset.\n","\n","    Args:\n","        model_name (str): Model name. It can be \"model\", \"modelMeta\", \"modeResnet18\", \"modeResnet101\", \"modelResnet152\"\n","        model_dir (str): Folder where the model is stored.\n","        map_loc (str): Device where the model will be loaded. It can be \"cpu\" or \"cuda\"\n","\n","    Returns:\n","        _type_: _description_\n","    \"\"\"\n","    # print(model_name)\n","    vocab, ingrs_vocab, ingr_vocab_size, instrs_vocab_size, output_dim = get_vocab_pickle(model_name)\n","    args = get_parser()\n","    args.maxseqlen = 15\n","    args.ingrs_only=False\n","    # if metal model, use default arguments for image model\n","    if model_name != \"modelMeta\":\n","        args.image_model = model_dict[model_name].lower()\n","    model = get_model(args, ingr_vocab_size, instrs_vocab_size)\n","    model_path = os.path.join(model_dir, model_name+'/checkpoints/modelbest.ckpt')\n","    # print(model_path)\n","    model.load_state_dict(torch.load(model_path, map_location=map_loc))\n","    model.to(device)\n","    model.eval()\n","    model.ingrs_only = False\n","    model.recipe_only = False\n","\n","    return model, vocab, ingrs_vocab, ingr_vocab_size, instrs_vocab_size, output_dim"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1702060673817,"user":{"displayName":"juan rodrigo reyes","userId":"17113265916897419484"},"user_tz":300},"id":"QOkVA6RonC5R"},"outputs":[],"source":["# for model_name in model_list:\n","#     print(model_name)\n","#     model = load_model(model_name, model_dir, map_loc)\n","#     print(model)\n"]},{"cell_type":"markdown","metadata":{"id":"leVNm8-SnC5S"},"source":["### get predictions on the datasets"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1702060673817,"user":{"displayName":"juan rodrigo reyes","userId":"17113265916897419484"},"user_tz":300},"id":"1W3z9fFPnC5S"},"outputs":[],"source":["transf_list_batch = []\n","transf_list_batch.append(transforms.ToTensor())\n","transf_list_batch.append(transforms.Normalize((0.485, 0.456, 0.406),\n","                                              (0.229, 0.224, 0.225)))\n","to_input_transf = transforms.Compose(transf_list_batch)\n","\n","greedy = True\n","beam =-1\n","temperature = 1.0\n","numgens = 1\n","\n"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":502,"status":"ok","timestamp":1702060674743,"user":{"displayName":"juan rodrigo reyes","userId":"17113265916897419484"},"user_tz":300},"id":"PWWarCbMnC5T"},"outputs":[],"source":["def generate_predictions(model_name,dataset,model_dir, map_loc):\n","    \"\"\"Function to generate predictions given a model name and a dataset.\n","    The model name is the name of the model to be loaded.\n","    The dataset is the dataset to be used to generate the predictions.\n","    The function will return a list of lists with the following structure:\n","    [dataset, model_name, img_file, greedy,beam, outs['title'], outs['ingrs'], outs['recipe']]\n","    The list of lists will contain the dataset name, the model name, the image file name,\n","    the greedy flag, the beam size, the title, the ingredients and the recipe.\n","\n","    Args:\n","        model_name (str): Model name. It can be \"model\", \"modelMeta\", \"modeResnet18\", \"modeResnet101\", \"modelResnet152\"\n","        dataset (str): Dataset to be used. It can be \"train\", \"test\" or \"val\"\n","    \"\"\"\n","    t = time.time()\n","    if dataset == \"train\":\n","        set_imgs = train_imgs\n","        image_folder = train_image_folder\n","    elif dataset == \"test\":\n","        set_imgs = test_imgs\n","        image_folder = test_image_folder\n","    elif dataset == \"val\":\n","        set_imgs = val_imgs\n","        image_folder = val_image_folder\n","    else:\n","        print(\"Dataset not valid\")\n","        return None\n","\n","    model, vocab, ingrs_vocab, ingr_vocab_size, instrs_vocab_size, output_dim = load_model(model_name, model_dir, map_loc)\n","    print(\"model loaded successfully\")\n","    predictions = []\n","    for img_num, img_file in enumerate(set_imgs):\n","        image_path = os.path.join(image_folder, img_file)\n","        image = Image.open(image_path).convert('RGB')\n","\n","        transf_list = []\n","        transf_list.append(transforms.Resize(256))\n","        transf_list.append(transforms.CenterCrop(224))\n","        transform = transforms.Compose(transf_list)\n","\n","        image_transf = transform(image)\n","        image_tensor = to_input_transf(image_transf).unsqueeze(0).to(device)\n","        # plt.imshow(image_transf)\n","        # plt.axis('off')\n","        # plt.show()\n","        # plt.close()\n","\n","        pred_valid = False\n","        counter_not_valid = 0\n","\n","        while not pred_valid:\n","            with torch.no_grad():\n","                outputs = model.sample(image_tensor, greedy=greedy,\n","                                    temperature=temperature, beam=beam, true_ingrs=None)\n","\n","            ingr_ids = outputs['ingr_ids'].cpu().numpy()\n","            recipe_ids = outputs['recipe_ids'].cpu().numpy()\n","\n","            outs, valid = prepare_output(recipe_ids[0], ingr_ids[0], ingrs_vocab, vocab)\n","            # check if output is valid, if not try again until a valid output is found\n","            # or try again 5 times and then return the last valid output\n","            if valid['is_valid'] or counter_not_valid < 5:\n","\n","                predictions.append([dataset, model_name, model_dict[model_name], img_file, greedy,beam, outs['title'], outs['ingrs'], outs['recipe']])\n","\n","                pred_valid=True\n","\n","            else:\n","                pass\n","                print (\"Not a valid recipe!\")\n","                print (\"Reason: \", valid['reason'])\n","                print( outs['title'], outs['ingrs'])\n","                counter_not_valid += 1\n","        if img_num % 100 == 0 and img_num >0:\n","            print(img_num)\n","            print(\"Elapsed time:\", time.time() -t)\n","            columns = ['dataset', 'model_name', 'conv_model', 'img_file', 'greedy','beam', 'title', 'ingrs', 'recipe']\n","            back_up_predictions = pd.DataFrame(predictions, columns=columns)\n","            back_up_predictions.to_csv(f'../data/predictions_local/{dataset}_{model_name}_{img_num}.csv', index=False)\n","\n","    return predictions"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/plain":["['modelMeta', 'modelResnet152', 'modeResnet101', 'modeResnet18', 'model']"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["reversed_model_list = [x for x in reversed(model_list)]\n","reversed_model_list"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VsLw2EbYnC5T","outputId":"006b6c58-e8cd-4b68-acc5-200469dd6149"},"outputs":[{"name":"stdout","output_type":"stream","text":["test\n","modelMeta\n","model loaded successfully\n","100\n","Elapsed time: 1641.9044303894043\n","200\n","Elapsed time: 2765.5693843364716\n","300\n","Elapsed time: 3488.649792909622\n","400\n","Elapsed time: 4220.983643293381\n","500\n","Elapsed time: 4861.877682924271\n","600\n","Elapsed time: 5496.561667203903\n","700\n","Elapsed time: 6130.855847358704\n","800\n","Elapsed time: 6765.481173276901\n","900\n","Elapsed time: 7499.807821035385\n","1000\n","Elapsed time: 8209.963100910187\n","1100\n","Elapsed time: 8864.133524179459\n","1200\n","Elapsed time: 10008.753876209259\n","1300\n","Elapsed time: 11131.933825969696\n","1400\n","Elapsed time: 12101.945229768753\n","1500\n","Elapsed time: 13147.195094108582\n","1600\n","Elapsed time: 14088.630777835846\n","1700\n","Elapsed time: 14961.173151493073\n","1800\n","Elapsed time: 15624.235769987106\n","1900\n","Elapsed time: 16252.48830127716\n","2000\n","Elapsed time: 16879.31926202774\n","modelResnet152\n","model loaded successfully\n","100\n","Elapsed time: 612.6885900497437\n","200\n","Elapsed time: 1250.5965127944946\n","300\n","Elapsed time: 1900.1866707801819\n","400\n","Elapsed time: 2480.6609156131744\n","500\n","Elapsed time: 3064.3737783432007\n","600\n","Elapsed time: 3652.160734653473\n","700\n","Elapsed time: 4237.276046514511\n","800\n","Elapsed time: 4898.238321065903\n","900\n","Elapsed time: 5573.901138305664\n","1000\n","Elapsed time: 6151.634135484695\n","1100\n","Elapsed time: 6740.200386285782\n","1200\n","Elapsed time: 7319.686975002289\n","1300\n","Elapsed time: 7898.720840454102\n","1400\n","Elapsed time: 8495.677018642426\n","1500\n","Elapsed time: 9152.977595090866\n","1600\n","Elapsed time: 9744.638590812683\n","1700\n","Elapsed time: 10320.448751688004\n","1800\n","Elapsed time: 10948.506283283234\n","1900\n","Elapsed time: 11534.585651159286\n","2000\n","Elapsed time: 12158.04047703743\n","modeResnet101\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\JREYES\\Anaconda3\\envs\\cs7643-a4\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["model loaded successfully\n","100\n","Elapsed time: 603.0318629741669\n","200\n","Elapsed time: 1169.3915450572968\n","300\n","Elapsed time: 1745.07040476799\n","400\n","Elapsed time: 2319.768133878708\n","500\n","Elapsed time: 2891.4208538532257\n","600\n","Elapsed time: 3496.814564704895\n","700\n","Elapsed time: 4136.98327755928\n","800\n","Elapsed time: 4727.953071594238\n","900\n","Elapsed time: 5298.123396873474\n","1000\n","Elapsed time: 5865.538738012314\n","1100\n","Elapsed time: 6430.157707452774\n","1200\n","Elapsed time: 7048.933167695999\n","1300\n","Elapsed time: 7695.4416971206665\n","1400\n","Elapsed time: 8261.526009321213\n","1500\n","Elapsed time: 8837.678347110748\n","1600\n","Elapsed time: 9426.369873285294\n","1700\n","Elapsed time: 9997.00268483162\n","1800\n","Elapsed time: 10586.217996358871\n","1900\n","Elapsed time: 11292.376510858536\n","2000\n","Elapsed time: 11874.133663415909\n","modeResnet18\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\JREYES\\Anaconda3\\envs\\cs7643-a4\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["model loaded successfully\n","100\n","Elapsed time: 576.0406672954559\n","200\n","Elapsed time: 1132.4753391742706\n","300\n","Elapsed time: 1692.3500254154205\n","400\n","Elapsed time: 2255.459806203842\n","500\n","Elapsed time: 2874.063105583191\n","600\n","Elapsed time: 3440.0612468719482\n","700\n","Elapsed time: 4004.5476603507996\n","800\n","Elapsed time: 4566.752469539642\n","900\n","Elapsed time: 5127.404501199722\n","1000\n","Elapsed time: 5700.027761936188\n","1100\n","Elapsed time: 6375.942176580429\n","1200\n","Elapsed time: 7027.802142381668\n","1300\n","Elapsed time: 7749.431973695755\n","1400\n","Elapsed time: 8308.107757806778\n","1500\n","Elapsed time: 8872.692833662033\n","1600\n","Elapsed time: 9446.417455911636\n","1700\n","Elapsed time: 10077.90397644043\n","1800\n","Elapsed time: 11172.546411037445\n","1900\n","Elapsed time: 12047.051998138428\n","2000\n","Elapsed time: 13271.309714078903\n","model\n","model loaded successfully\n","100\n","Elapsed time: 916.099939584732\n","200\n","Elapsed time: 1988.9499776363373\n","300\n","Elapsed time: 2676.0341517925262\n","400\n","Elapsed time: 4030.3481090068817\n","500\n","Elapsed time: 5352.137170553207\n","600\n","Elapsed time: 5956.789909362793\n","700\n","Elapsed time: 6519.305432796478\n","800\n","Elapsed time: 7172.913377046585\n","900\n","Elapsed time: 7749.549434185028\n","1000\n","Elapsed time: 8322.866292953491\n","1100\n","Elapsed time: 8903.832891702652\n","1200\n","Elapsed time: 9478.890634059906\n","1300\n","Elapsed time: 10061.610813379288\n","1400\n","Elapsed time: 10718.804260015488\n","1500\n","Elapsed time: 11311.931410551071\n","1600\n","Elapsed time: 11884.04583120346\n","1700\n","Elapsed time: 12909.952441215515\n","1800\n","Elapsed time: 14298.41742682457\n","1900\n","Elapsed time: 15285.16938495636\n","2000\n","Elapsed time: 15858.993254899979\n"]}],"source":["for dataset in [\"test\"]:#,\"val\",\"train\" ]:\n","    print(dataset)\n","    for model_name in reversed_model_list:\n","        print(model_name)\n","        predictions = generate_predictions(model_name,dataset,model_dir, map_loc)\n","        columns = ['dataset', 'model_name', 'conv_model', 'img_file', 'greedy','beam', 'title', 'ingrs', 'recipe']\n","        predictions = pd.DataFrame(predictions, columns=columns)\n","        predictions.to_csv(f'../data/predictions_local/{dataset}_{model_name}.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8mmI9YoDn7eK"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_QyVE8oBn7o_"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gXys0YIrnC5U"},"outputs":[],"source":["t = time.time()\n","import sys; sys.argv=['']; del sys\n","args = get_parser()\n","args.maxseqlen = 15\n","args.ingrs_only=False\n","model = get_model(args, ingr_vocab_size, instrs_vocab_size)\n","# Load the trained model parameters\n","model_path = os.path.join(data_dir, 'modelbest.ckpt')\n","model.load_state_dict(torch.load(model_path, map_location=map_loc))\n","model.to(device)\n","model.eval()\n","model.ingrs_only = False\n","model.recipe_only = False\n","print ('loaded model')\n","print (\"Elapsed time:\", time.time() -t)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pp7I0VAynC5U"},"outputs":[],"source":["transf_list_batch = []\n","transf_list_batch.append(transforms.ToTensor())\n","transf_list_batch.append(transforms.Normalize((0.485, 0.456, 0.406),\n","                                              (0.229, 0.224, 0.225)))\n","to_input_transf = transforms.Compose(transf_list_batch)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MDkRQewNnC5U"},"outputs":[],"source":["greedy = [True, False, False, False]\n","beam = [-1, -1, -1, -1]\n","temperature = 1.0\n","numgens = len(greedy)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yOKTEKVrnC5V"},"outputs":[],"source":["show_anyways = False\n","image_folder = os.path.join(data_dir, 'demo_imgs')\n","\n","demo_imgs = os.listdir(image_folder)\n","demo_files = demo_imgs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mUrBxSIJnC5V"},"outputs":[],"source":["for img_file in demo_files:\n","\n","    image_path = os.path.join(image_folder, img_file)\n","    image = Image.open(image_path).convert('RGB')\n","\n","    transf_list = []\n","    transf_list.append(transforms.Resize(256))\n","    transf_list.append(transforms.CenterCrop(224))\n","    transform = transforms.Compose(transf_list)\n","\n","    image_transf = transform(image)\n","    image_tensor = to_input_transf(image_transf).unsqueeze(0).to(device)\n","\n","    plt.imshow(image_transf)\n","    plt.axis('off')\n","    plt.show()\n","    plt.close()\n","\n","    num_valid = 1\n","    for i in range(numgens):\n","        with torch.no_grad():\n","            outputs = model.sample(image_tensor, greedy=greedy[i],\n","                                   temperature=temperature, beam=beam[i], true_ingrs=None)\n","\n","        ingr_ids = outputs['ingr_ids'].cpu().numpy()\n","        recipe_ids = outputs['recipe_ids'].cpu().numpy()\n","\n","        outs, valid = prepare_output(recipe_ids[0], ingr_ids[0], ingrs_vocab, vocab)\n","\n","        if valid['is_valid'] or show_anyways:\n","\n","            print ('RECIPE', num_valid)\n","            num_valid+=1\n","            #print (\"greedy:\", greedy[i], \"beam:\", beam[i])\n","\n","            BOLD = '\\033[1m'\n","            END = '\\033[0m'\n","            print (BOLD + '\\nTitle:' + END,outs['title'])\n","\n","            print (BOLD + '\\nIngredients:'+ END)\n","            print (', '.join(outs['ingrs']))\n","\n","            print (BOLD + '\\nInstructions:'+END)\n","            print ('-'+'\\n-'.join(outs['recipe']))\n","\n","            print ('='*20)\n","\n","        else:\n","            pass\n","            print (\"Not a valid recipe!\")\n","            print (\"Reason: \", valid['reason'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3fp4Fvu7nC5W"},"outputs":[],"source":["train_image_folder = os.path.join(\"../Kaggle data/images\", 'train')\n","test_image_folder = os.path.join(\"../Kaggle data/images\", 'test')\n","val_image_folder = os.path.join(\"../Kaggle data/images\", 'val')\n","\n","train_imgs = os.listdir(train_image_folder)\n","test_imgs = os.listdir(test_image_folder)\n","val_imgs = os.listdir(val_image_folder)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N7J4TnKmnC5W"},"outputs":[],"source":["train_imgs[:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2eycQI7WnC5X"},"outputs":[],"source":["for img_num, img_file in enumerate(val_imgs[:5]):\n","    if img_num % 100:\n","        print(img_num)\n","    image_path = os.path.join(val_image_folder, img_file)\n","    image = Image.open(image_path).convert('RGB')\n","\n","    transf_list = []\n","    transf_list.append(transforms.Resize(256))\n","    transf_list.append(transforms.CenterCrop(224))\n","    transform = transforms.Compose(transf_list)\n","\n","    image_transf = transform(image)\n","    image_tensor = to_input_transf(image_transf).unsqueeze(0).to(device)\n","    plt.imshow(image_transf)\n","    plt.axis('off')\n","    plt.show()\n","    plt.close()\n","\n","    num_valid = 1\n","    for i in range(numgens):\n","        with torch.no_grad():\n","            outputs = model.sample(image_tensor, greedy=greedy[i],\n","                                   temperature=temperature, beam=beam[i], true_ingrs=None)\n","\n","        ingr_ids = outputs['ingr_ids'].cpu().numpy()\n","        recipe_ids = outputs['recipe_ids'].cpu().numpy()\n","\n","        outs, valid = prepare_output(recipe_ids[0], ingr_ids[0], ingrs_vocab, vocab)\n","\n","        if valid['is_valid'] or show_anyways:\n","\n","            print ('RECIPE', num_valid)\n","            num_valid+=1\n","            #print (\"greedy:\", greedy[i], \"beam:\", beam[i])\n","\n","            BOLD = '\\033[1m'\n","            END = '\\033[0m'\n","            print (BOLD + '\\nTitle:' + END,outs['title'])\n","\n","            print (BOLD + '\\nIngredients:'+ END)\n","            print (', '.join(outs['ingrs']))\n","\n","            print (BOLD + '\\nInstructions:'+END)\n","            print ('-'+'\\n-'.join(outs['recipe']))\n","\n","            print ('='*20)\n","\n","        else:\n","            pass\n","            print (\"Not a valid recipe!\")\n","            print (\"Reason: \", valid['reason'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Z4Ki-ANnC5X"},"outputs":[],"source":["outs"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
